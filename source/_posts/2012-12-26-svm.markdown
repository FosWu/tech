---
layout: post
title: "支持向量机"
date: 2012-12-26 19:40
comments: true
categories: Machine&nbspLearning
math: true
abstract: 
---

是今年工作中才开始接触机器学习的，之前有所听说，但是也没有深入了解过。其实所谓的接触主要是照着李航的这本[《统计学习方法》](http://book.douban.com/subject/10590856/)学习。当时我们是几个同事每人学习一章，学完了，然后给大家办个讲座。我想既然学习了，就应该写博客把这些内容记录下来。于是，就开始写了。

机器学习是最近十年兴起的一门学科。人工智能是计算机学界的一个公认的难题，而机器学习被认为是最有可能解决这个难题的一门学科。当然机器学习在其它很多领域都有应用，比如数据挖掘，信息检索，语言识别，图像识别等很多领域。对于机器学习的基本概念，可以看[《统计学习方法》](http://book.douban.com/subject/10590856/)的第一章，发展历史可以看wikipedia的[Machine learning](http://en.wikipedia.org/wiki/Machine_learning)。我这里就不罗嗦了。等以后再写一篇机器学习的综述文章吧。

按照[《统计学习方法》](http://book.douban.com/subject/10590856/)的划分，机器学习可以分为监督学习，无监督学习和半监督学习、强化学习等。该书讲了监督学习，总共讲了这些学习模型：感知机、k近邻法、朴素贝叶斯法、决策树、逻辑斯谛回归、最大熵、支持向量机、EM算法、隐马尔科夫模型、条件随机场。我先把我们学习过的模型写成博客吧。但其实每个模型都有很多内容，以我一个初学者的水平估计也讲不了什么，我就按照我的理解讲，每个模型争取能实现一个例子，供大家参考。

今天第一篇，SVM（support vector machine, 支持向量机）。

<!-- more -->

## 线性支持向量机

SVM是一个分类器，而且还只能二分，也就是回答yes or no的问题。先用图说话，

{% img center /images/blogimages/2012/svm/samples.png %}

上面的图中的红点和绿点分布代表不同的两个类别，很明显了可以用一条直线将它们分开, 例如下面的蓝线。

{% img center /images/blogimages/2012/svm/samples_line.png %}

上面的蓝线就是一种分类方法，当然你也可以画一条曲线，或者用一个圆圈把红点包围起来，这些都可以。用一条直线划分，就是线性的划分方式。这样的线性划分方式，可以用感知机，也可以用线性支持向量进行划分。(关于感知机，可以参考[《统计学习方法》](http://book.douban.com/subject/10590856/)的第二章，这里就不多讲了。

其实这样的直线有很多条，例如下图中的蓝线、红线、黄线，都是可以。

{% img center /images/blogimages/2012/svm/samples_3lines.png %}

那么svm是按照什么原则确定哪条划分的直线呢？svm的原则是“找到一条直线，把所有的样本点尽量分开”，换句话说，在一类样本点中，离划分直线最近点，到划分直接的距离要越大越好，那些点就被称之为支撑向量(surpport vector)。支持向量机的学习目标，就是找到那样的一条划分直线。例如下图中的蓝色实线，就是一个支持向量机。

{% img center /images/blogimages/2012/svm/svm_sep.png %}

图中在蓝色虚线上的3个点，就是支持向量。现在来理解一下“距离越大越好”，这个距离是指的支持向量到划分直线的距离，对应图中的粉色线条，并不是说的两个支持向量直接的距离，即非图中的绿色线条。

以上就是一个简单的svm的原理模型。稍微扩展一下，上面的例子中，所有的点都是二维的，那么就可以用一条直线把它们划分开，对于样本点是多维的情况，那么支持向量机算出来的是一个超平面，也称之为间隔分离超平面。

可以将上面的问题，抽象化，即最大化间隔分离超平面，可以用下面的公式表示(推导见[《统计学习方法》](http://book.douban.com/subject/10590856/)的7.1)：

$$
\begin{array}{l}
\mathop {\max }\limits_{w,b} \qquad \frac{\hat \gamma }{\left\| w \right\|}\\
{\rm{s.t.}} \qquad {y_i}(w\cdot{x_i} + b) \ge \hat \gamma 
\end{array}
$$

上面的公式中，$\hat \gamma$表示样本点到分离超平面的距离，${x_i}$为样本点，${y_i}$为样本点的分类，这里一般取1和-1。

可以令$\hat \gamma  = 1$，另最大化$\frac{1}{\left \\| w \right \\|}$与最小化$\frac{1}{2}\left\\| w \right\\|^2$是等价的，所以上面的公式可以变化为：

$$
\begin{array}{l}
\mathop {\max }\limits_{w,b} \qquad \frac{1}{2}\left\| w \right\|^2\\
{\rm{s.t.}} \qquad {y_i}(w\cdot{x_i} + b) - 1 \ge 0
\end{array}
$$


求解上面的问题，可以通过建立拉格朗日函数，并通过对偶变化，最终将问题转换成求解下面的公式：

$$
\begin{array}{l}
\mathop {\min }\limits_a \qquad \frac{1}{2}\sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N { {a_i}{a_j}{y_i}{y_j}({x_i}\cdot{x_j}) - \sum\limits_{i = 1}^N { {a_i}} } } \\
{\rm{s.t.}}\qquad\sum\limits_{i = 1}^N { {a_i}{y_i} = 0} \\
\qquad\qquad{a_i} \ge 0, \qquad i = 1,2,...,N
\end{array}
$$



对于上面的例子，可以用一条直线或者说一个超平面将训练集分开，那么对应这个问题
