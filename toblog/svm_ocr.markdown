
## 介绍svm的多类分类问题
http://www.blogjava.net/zhenandaci/archive/2009/03/26/262113.html
1. 一对多（一对其余），比如我们有5个类别，第一次就把类别1的样本定为正样本，其余2，3，4，5的样本合起来定为负样本，这样得到一个两类分类器。通过类似的方法构造类别2、3、4、5的分类器，对于测试数据，每一个分类器都过一次，在那个分类器中，被判定为正，那么就认识它属于那个分类。但有两个问题还没有解决：一是被多个分类器判定为正，二是被所有分类器判定为负。第一种情况，可以简单地选择第一个被判定为正的分类，对二种情况，可以视为分类失败了。另外一个问题是，“其余”的那一类样本数总是要数倍于正类（因为它是除正类以外其他类别的样本之和嘛），这就人为的造成了的[“数据集偏斜”](http://www.blogjava.net/zhenandaci/archive/2009/03/17/260315.html)问题。
2，一对一, 这需要构造n\*(n-1)/2个分类器。简单地可以理解为所有分类1v1进行pk,具体做法，还是每次选一个类的样本作正类样本，而负类样本则变成只选一个类，(例如构造一个1 pk 2的分类器，它的正样本为1的分类，负样本为2的分类）这就避免了偏斜。因此过程就是算出这样一些分类器，第一个只回答“是第1类还是第2类”，第二个只回答“是第1类还是第3类”，第三个只回答“是第1类还是第4类”，如此下去，所以一共有5\*(5-1)/2=10个分类器。虽然分类器的数目多了，但是在训练阶段（也就是算出这些分类器的分类平面时）所用的总时间却比“一类对其余”方法少很多，在真正用来分类的时候，把一个测试数据扔给所有分类器，第一个分类器会投票说它是“1”或者“2”，第二个会说它是“1”或者“3”，让每一个都投上自己的一票，最后统计票数，如果类别“1”得票最多，就判这篇文章属于第1类。但这有个问题，分类器的数量是类别数量的平方,例如，类别数如果是1000，要调用的分类器数目会上升至约500,000个（类别数的平方量级）。
3. 构造一个DAG SVM，（有向无环的svm)。还是像一对一方法那样来训练，只是在对一篇文章进行分类之前，先按照下面图的样子来组织分类器

这样在分类时,我们就可以先问分类器“1对5”（意思是它能够回答“是第1类还是第5类”），如果它回答5，我们就往左走，再问“2对5”这个分类器，如果它还说是“5”，我们就继续往左走，这样一直问下去，就可以得到分类结果。好处在哪？我们其实只调用了4个分类器（如果类别数是k，则只调用k-1个），分类速度飞快，且没有分类重叠和不可分类现象！缺点在哪？假如最一开始的分类器回答错误（明明是类别1的文章，它说成了5），那么后面的分类器是无论如何也无法纠正它的错误的（因为后面的分类器压根没有出现“1”这个类别标签），其实对下面每一层的分类器都存在这种错误向下累积的现象。。

不过不要被DAG方法的错误累积吓倒，错误累积在一对其余和一对一方法中也都存在，DAG方法好于它们的地方就在于，累积的上限，不管是大是小，总是有定论的，有理论证明。而一对其余和一对一方法中，尽管每一个两类分类器的泛化误差限是知道的，但是合起来做多类分类的时候，误差上界是多少，没人知道，这意味着准确率低到0也是有可能的，这多让人郁闷。

而且现在DAG方法根节点的选取（也就是如何选第一个参与分类的分类器），也有一些方法可以改善整体效果，我们总希望根节点少犯错误为好，因此参与第一次分类的两个类别，最好是差别特别特别大，大到以至于不太可能把他们分错；或者我们就总取在两类分类中正确率最高的那个分类器作根节点，或者我们让两类分类器在分类的时候，不光输出类别的标签，还输出一个类似“置信度”的东东，当它对自己的结果不太自信的时候，我们就不光按照它的输出走，把它旁边的那条路也走一走，等等。 


## 高斯核函数的 
高斯径向基函数的宽度

σ对分类器的性能比较敏感，文献[3]对取不同的σ时，高斯核支持向量机的性能进行分析，若0→σ，则所有的训练样本点都是支持向量，且它们全部能被正确的分类，但容易出现“过学习”的现象，推广能力较差，对测试样本的错误识别率较高；若∞→σ，高斯核支持向量机对所有样本一视同仁，推广能力或对测试样本的正确判别能力为零，即它把所有样本点判为同一类。
实际上，当σ取比训练样本点之间的平均距离小得多时，就能达到0→σ的效果；当σ取比训练样本点之间的平均距离大得多时，就能达到∞→σ的效果
在确定高斯径向基函数的宽度σ时，最基本的方法是对σ取不同的值，然后分别采用支持向量机方法进行训练，选择最小分类错误率的一组σ参数。比较典型的方法有梯度下降法[4]与交叉验证法[5]

参考文献：
张翔，肖小玲，徐光祐，一种确定高斯核模型参数的新方法，计算机工程，第33卷，第12期，2007年6月

## 高斯核函数
本文中，cache了高斯核函数的结算结果

## 实现过程中的问题
一开始效率很地，两个地方的改进
cache核函数的运算结果
计算预测值的与真实值之差Ei，改用增量变化的方式，即每次只计算出new\_ai, new\_aj对Ei的改变量。

识别率低，通过配置精度，惩罚系数，ai改变步长，提高识别率
dlt的调整最有效果，最优值，是当dlt=10左右。

## 本文例子中的问题
1. 样本数据的偏移，训练时，正负样本的数量应该是相当的。本文中的例子，负样本多余正样本。
2. 高斯径向基函数的宽度，没有经过训练
3. 分类器的组织按照一对多的方式。 
4. 选择第二个变量时，本文使用的是遍历方式，也可以改成寻找改变量最大的第二个参数，具体参考书的P129
5. 核函数计算时，可以如果能用到位运算，应该效率会高很多。


公式

${E_i}$为函数$g({x_i})$对输入的${x_i}$的预测值与真实值${y_i}$之差。代码中，对应了`predict_diff_real()`函数

${E_i} = g({x_i}) - {y_i} = (\sum\limits_{j = 1}^N {{a_j}{y_j}K({x_j},{x_i}) + b) - {y_i}} $



